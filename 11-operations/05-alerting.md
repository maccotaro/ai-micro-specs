# ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**æœ€çµ‚æ›´æ–°**: 2025-09-30
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… ç¢ºå®š

## æ¦‚è¦

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ai-micro-service ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ã‚¢ãƒ©ãƒ¼ãƒˆæˆ¦ç•¥ã€ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ã€é€šçŸ¥è¨­å®šã«ã¤ã„ã¦å®šç¾©ã—ã¾ã™ã€‚

## ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆã®åŸå‰‡

### åŸºæœ¬æ–¹é‡

1. **Actionableï¼ˆå¯¾å¿œå¯èƒ½ï¼‰**: ã‚¢ãƒ©ãƒ¼ãƒˆã«å¯¾ã—ã¦æ˜ç¢ºãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚‹
2. **Meaningfulï¼ˆæ„å‘³ãŒã‚ã‚‹ï¼‰**: ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿ãŒã‚ã‚‹
3. **Low False Positiveï¼ˆèª¤æ¤œçŸ¥ãŒå°‘ãªã„ï¼‰**: ä¿¡é ¼æ€§ãŒé«˜ã„
4. **Prioritizedï¼ˆå„ªå…ˆé †ä½ä»˜ã‘ï¼‰**: é‡è¦åº¦ã«å¿œã˜ãŸéšå±¤åŒ–
5. **Well-Documentedï¼ˆæ–‡æ›¸åŒ–ï¼‰**: å¯¾å¿œæ‰‹é †ãŒæ˜ç¢º

### ã‚¢ãƒ©ãƒ¼ãƒˆç–²ã‚Œï¼ˆAlert Fatigueï¼‰ã®é˜²æ­¢

```
âŒ é¿ã‘ã‚‹ã¹ãã‚¢ãƒ©ãƒ¼ãƒˆ:
  - å¯¾å¿œä¸è¦ãªã‚¢ãƒ©ãƒ¼ãƒˆ
  - é »ç¹ã™ãã‚‹ã‚¢ãƒ©ãƒ¼ãƒˆ
  - ä¸æ˜ç­ãªã‚¢ãƒ©ãƒ¼ãƒˆ

âœ… è‰¯ã„ã‚¢ãƒ©ãƒ¼ãƒˆ:
  - ãƒ¦ãƒ¼ã‚¶ãƒ¼å½±éŸ¿ãŒã‚ã‚‹ã‚‚ã®
  - ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ãªã‚‚ã®
  - æ˜ç¢ºãªå¯¾å¿œæ‰‹é †ãŒã‚ã‚‹ã‚‚ã®
```

---

## ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«

### ãƒ¬ãƒ™ãƒ«å®šç¾©

| ãƒ¬ãƒ™ãƒ« | æ·±åˆ»åº¦ | å¯¾å¿œæ™‚é–“ | é€šçŸ¥æ–¹æ³• | ä¾‹ |
|--------|--------|----------|----------|-----|
| **CRITICAL** | è‡´å‘½çš„ | å³åº§ï¼ˆ5åˆ†ä»¥å†…ï¼‰ | PagerDuty + Slack + SMS | ã‚µãƒ¼ãƒ“ã‚¹å…¨åœæ­¢ |
| **ERROR** | ã‚¨ãƒ©ãƒ¼ | ç·Šæ€¥ï¼ˆ30åˆ†ä»¥å†…ï¼‰ | Slack + Email | ã‚¨ãƒ©ãƒ¼ç‡ > 5% |
| **WARNING** | è­¦å‘Š | é€šå¸¸ï¼ˆ4æ™‚é–“ä»¥å†…ï¼‰ | Slack | ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ > 80% |
| **INFO** | æƒ…å ± | é€šçŸ¥ã®ã¿ | Slack | ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº† |

### ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```
CRITICAL ã‚¢ãƒ©ãƒ¼ãƒˆ:
  1. å³åº§ã«ä¸€æ¬¡å¯¾å¿œãƒãƒ¼ãƒ ã«é€šçŸ¥
  2. 15åˆ†å¿œç­”ãªã— â†’ äºŒæ¬¡å¯¾å¿œãƒãƒ¼ãƒ ã«é€šçŸ¥
  3. 30åˆ†æœªè§£æ±º â†’ ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«é€šçŸ¥

ERROR ã‚¢ãƒ©ãƒ¼ãƒˆ:
  1. ä¸€æ¬¡å¯¾å¿œãƒãƒ¼ãƒ ã«é€šçŸ¥
  2. 1æ™‚é–“æœªè§£æ±º â†’ ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

WARNING ã‚¢ãƒ©ãƒ¼ãƒˆ:
  1. Slacké€šçŸ¥ã®ã¿
  2. 24æ™‚é–“ç¶™ç¶š â†’ ãƒã‚±ãƒƒãƒˆä½œæˆ
```

---

## AlertManager è¨­å®š

### åŸºæœ¬è¨­å®š

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# ãƒ«ãƒ¼ãƒˆãƒ«ãƒ¼ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
route:
  receiver: 'slack-notifications'
  group_by: ['alertname', 'service', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h

  # ã‚µãƒ–ãƒ«ãƒ¼ãƒˆï¼ˆé‡è¦åº¦åˆ¥ï¼‰
  routes:
    # CRITICAL: PagerDuty + Slack
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 0s
      repeat_interval: 5m

    # ERROR: Slack + Email
    - match:
        severity: error
      receiver: 'slack-email-notifications'
      repeat_interval: 1h

    # WARNING: Slack ã®ã¿
    - match:
        severity: warning
      receiver: 'slack-notifications'
      repeat_interval: 4h

# Receiversï¼ˆé€šçŸ¥å…ˆï¼‰
receivers:
  # Slacké€šçŸ¥
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: |
          *Severity*: {{ .GroupLabels.severity }}
          *Service*: {{ .GroupLabels.service }}
          *Summary*: {{ .CommonAnnotations.summary }}
          *Description*: {{ .CommonAnnotations.description }}

  # Slack + Email
  - name: 'slack-email-notifications'
    slack_configs:
      - channel: '#alerts-critical'
        title: 'ğŸš¨ {{ .GroupLabels.alertname }}'
        text: |
          *Severity*: ERROR
          *Service*: {{ .GroupLabels.service }}
          *Summary*: {{ .CommonAnnotations.summary }}
          *Runbook*: {{ .CommonAnnotations.runbook_url }}
    email_configs:
      - to: 'oncall@example.com'
        from: 'alertmanager@example.com'
        subject: 'ALERT: {{ .GroupLabels.alertname }}'

  # PagerDutyï¼ˆCRITICALï¼‰
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        description: '{{ .CommonAnnotations.summary }}'
    slack_configs:
      - channel: '#incidents'
        title: 'ğŸ”´ CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          *Service*: {{ .GroupLabels.service }}
          *Summary*: {{ .CommonAnnotations.summary }}
          *PagerDuty*: Incident created

# Inhibition rulesï¼ˆæŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«ï¼‰
inhibit_rules:
  # CRITICALã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç«ä¸­ã¯WARNINGã‚’æŠ‘åˆ¶
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['service']

  # ã‚µãƒ¼ãƒ“ã‚¹ãƒ€ã‚¦ãƒ³ä¸­ã¯ä»–ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æŠ‘åˆ¶
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['service']
```

---

## ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«

### ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚¢ãƒ©ãƒ¼ãƒˆ

#### PostgreSQL

```yaml
# alerts/postgres.yml
groups:
  - name: postgres
    interval: 30s
    rules:
      # PostgreSQL ãƒ€ã‚¦ãƒ³
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          service: postgresql
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL instance {{ $labels.instance }} is down for more than 1 minute."
          runbook_url: "https://docs.example.com/runbooks/postgres-down"

      # æ¥ç¶šæ•°ãŒä¸Šé™ã«è¿‘ã„
      - alert: PostgreSQLTooManyConnections
        expr: |
          (pg_stat_database_numbackends / pg_settings_max_connections * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "PostgreSQL connection limit approaching"
          description: "PostgreSQL instance {{ $labels.instance }} is using {{ $value }}% of max connections."
          runbook_url: "https://docs.example.com/runbooks/postgres-connections"

      # ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãŒå¤šã„
      - alert: PostgreSQLSlowQueries
        expr: |
          rate(pg_stat_statements_mean_exec_time_seconds{query!~".*pg_stat.*"}[5m]) > 1
        for: 10m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "High number of slow queries detected"
          description: "Database {{ $labels.datname }} has queries with average execution time > 1s."

      # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºãŒå¤§ãã„
      - alert: PostgreSQLDatabaseSizeLarge
        expr: |
          (pg_database_size_bytes / 1024 / 1024 / 1024) > 50
        for: 1h
        labels:
          severity: info
          service: postgresql
        annotations:
          summary: "Database size is large"
          description: "Database {{ $labels.datname }} size is {{ $value }}GB."

      # ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é…å»¶ï¼ˆè©²å½“æ™‚ï¼‰
      - alert: PostgreSQLReplicationLag
        expr: |
          pg_replication_lag > 30
        for: 5m
        labels:
          severity: error
          service: postgresql
        annotations:
          summary: "PostgreSQL replication lag detected"
          description: "Replication lag is {{ $value }} seconds on {{ $labels.instance }}."
```

#### Redis

```yaml
# alerts/redis.yml
groups:
  - name: redis
    interval: 30s
    rules:
      # Redis ãƒ€ã‚¦ãƒ³
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          service: redis
        annotations:
          summary: "Redis is down"
          description: "Redis instance {{ $labels.instance }} is down."
          runbook_url: "https://docs.example.com/runbooks/redis-down"

      # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„
      - alert: RedisMemoryHigh
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Redis memory usage is high"
          description: "Redis instance {{ $labels.instance }} is using {{ $value }}% of max memory."

      # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãŒä½ã„
      - alert: RedisCacheHitRateLow
        expr: |
          (rate(redis_keyspace_hits_total[5m]) / (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])) * 100) < 60
        for: 15m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Redis cache hit rate is low"
          description: "Redis cache hit rate is {{ $value }}% on {{ $labels.instance }}."

      # æ¥ç¶šæ•°ãŒå¤šã„
      - alert: RedisTooManyConnections
        expr: redis_connected_clients > 100
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Too many Redis connections"
          description: "Redis has {{ $value }} connected clients on {{ $labels.instance }}."

      # ã‚­ãƒ¼å‰Šé™¤ãŒé »ç¹
      - alert: RedisHighEvictionRate
        expr: rate(redis_evicted_keys_total[5m]) > 10
        for: 10m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "High Redis key eviction rate"
          description: "Redis is evicting {{ $value }} keys/sec on {{ $labels.instance }}."
```

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ©ãƒ¼ãƒˆ

#### Auth Service

```yaml
# alerts/auth-service.yml
groups:
  - name: auth-service
    interval: 30s
    rules:
      # ã‚µãƒ¼ãƒ“ã‚¹ãƒ€ã‚¦ãƒ³
      - alert: AuthServiceDown
        expr: up{job="auth-service"} == 0
        for: 2m
        labels:
          severity: critical
          service: auth-service
        annotations:
          summary: "Auth Service is down"
          description: "Auth Service has been down for more than 2 minutes."
          runbook_url: "https://docs.example.com/runbooks/auth-service-down"

      # ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„
      - alert: AuthServiceHighErrorRate
        expr: |
          (sum(rate(http_requests_total{service="auth-service",status=~"5.."}[5m])) /
          sum(rate(http_requests_total{service="auth-service"}[5m])) * 100) > 5
        for: 5m
        labels:
          severity: error
          service: auth-service
        annotations:
          summary: "High error rate in Auth Service"
          description: "Auth Service error rate is {{ $value }}%."

      # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ãŒé…ã„
      - alert: AuthServiceHighLatency
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket{service="auth-service"}[5m])
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: auth-service
        annotations:
          summary: "High latency in Auth Service"
          description: "Auth Service p95 latency is {{ $value }}s."

      # ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ç‡ãŒé«˜ã„
      - alert: AuthServiceHighLoginFailureRate
        expr: |
          (rate(auth_login_failures_total[5m]) /
          rate(auth_login_attempts_total[5m]) * 100) > 30
        for: 5m
        labels:
          severity: warning
          service: auth-service
        annotations:
          summary: "High login failure rate"
          description: "Login failure rate is {{ $value }}%."

      # JWTæ¤œè¨¼å¤±æ•—ãŒå¤šã„
      - alert: AuthServiceJWTVerificationFailures
        expr: rate(jwt_verification_failures_total[5m]) > 10
        for: 5m
        labels:
          severity: error
          service: auth-service
        annotations:
          summary: "High JWT verification failure rate"
          description: "JWT verification failures: {{ $value }} per second."
```

#### User API & Admin API

```yaml
# alerts/api-services.yml
groups:
  - name: api-services
    interval: 30s
    rules:
      # ã‚µãƒ¼ãƒ“ã‚¹ãƒ€ã‚¦ãƒ³
      - alert: APIServiceDown
        expr: up{job=~"user-api|admin-api"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "{{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 2 minutes."

      # ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„
      - alert: APIServiceHighErrorRate
        expr: |
          (sum(rate(http_requests_total{job=~"user-api|admin-api",status=~"5.."}[5m])) by (job) /
          sum(rate(http_requests_total{job=~"user-api|admin-api"}[5m])) by (job) * 100) > 5
        for: 5m
        labels:
          severity: error
        annotations:
          summary: "High error rate in {{ $labels.job }}"
          description: "{{ $labels.job }} error rate is {{ $value }}%."

      # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ä¸­ãŒå¤šã„
      - alert: APIServiceHighConcurrency
        expr: http_requests_in_progress{job=~"user-api|admin-api"} > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High concurrent requests in {{ $labels.job }}"
          description: "{{ $labels.job }} has {{ $value }} requests in progress."

      # OCRå‡¦ç†æ™‚é–“ãŒé•·ã„ï¼ˆAdmin APIï¼‰
      - alert: AdminAPISlowOCRProcessing
        expr: |
          histogram_quantile(0.95,
            rate(ocr_processing_duration_seconds_bucket{job="admin-api"}[5m])
          ) > 30
        for: 10m
        labels:
          severity: warning
          service: admin-api
        annotations:
          summary: "Slow OCR processing"
          description: "OCR p95 processing time is {{ $value }}s."
```

### ã‚³ãƒ³ãƒ†ãƒŠã‚¢ãƒ©ãƒ¼ãƒˆ

```yaml
# alerts/containers.yml
groups:
  - name: containers
    interval: 30s
    rules:
      # CPUä½¿ç”¨ç‡ãŒé«˜ã„
      - alert: ContainerHighCPU
        expr: |
          (rate(container_cpu_usage_seconds_total{name=~".+"}[5m]) * 100) > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage in {{ $labels.name }}"
          description: "Container {{ $labels.name }} CPU usage is {{ $value }}%."

      # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„
      - alert: ContainerHighMemory
        expr: |
          (container_memory_usage_bytes{name=~".+"} /
          container_spec_memory_limit_bytes{name=~".+"} * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage in {{ $labels.name }}"
          description: "Container {{ $labels.name }} memory usage is {{ $value }}%."

      # ã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•ãŒé »ç¹
      - alert: ContainerFrequentRestarts
        expr: |
          rate(container_last_seen{name=~".+"}[10m]) > 0.1
        for: 5m
        labels:
          severity: error
        annotations:
          summary: "Frequent container restarts"
          description: "Container {{ $labels.name }} is restarting frequently."
```

---

## é€šçŸ¥è¨­å®š

### Slack é€šçŸ¥

#### Webhookè¨­å®š

```bash
# Slack Incoming Webhook URLã‚’å–å¾—
# 1. Slack Workspace ã§ Incoming Webhooks ã‚¢ãƒ—ãƒªã‚’è¿½åŠ 
# 2. Webhook URLã‚’ã‚³ãƒ”ãƒ¼
# 3. AlertManagerè¨­å®šã«è¿½åŠ 

export SLACK_WEBHOOK_URL="https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
```

#### ãƒãƒ£ãƒ³ãƒãƒ«è¨­è¨ˆ

```
#alerts: é€šå¸¸ã®ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆWARNING, INFOï¼‰
#alerts-critical: ERROR, CRITICAL ã‚¢ãƒ©ãƒ¼ãƒˆ
#incidents: CRITICAL ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå°‚ç”¨
#deployments: ãƒ‡ãƒ—ãƒ­ã‚¤é€šçŸ¥
```

#### ã‚«ã‚¹ã‚¿ãƒ Slacké€šçŸ¥

```python
# app/core/notifications.py
import httpx

SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

async def send_slack_alert(message: str, severity: str = "info"):
    emoji_map = {
        "critical": "ğŸ”´",
        "error": "ğŸŸ ",
        "warning": "ğŸŸ¡",
        "info": "ğŸ”µ"
    }

    payload = {
        "text": f"{emoji_map.get(severity, 'ğŸ”µ')} {message}",
        "blocks": [
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{severity.upper()}*: {message}"
                }
            }
        ]
    }

    async with httpx.AsyncClient() as client:
        await client.post(SLACK_WEBHOOK_URL, json=payload)

# ä½¿ç”¨ä¾‹
await send_slack_alert("Database connection failed", severity="error")
```

### Email é€šçŸ¥

```yaml
# alertmanager.yml
receivers:
  - name: 'email-notifications'
    email_configs:
      - to: 'team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: '${SMTP_PASSWORD}'
        headers:
          Subject: '[ALERT] {{ .GroupLabels.alertname }}'
```

### PagerDuty çµ±åˆ

```yaml
# alertmanager.yml
receivers:
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        url: 'https://events.pagerduty.com/v2/enqueue'
        description: '{{ .CommonAnnotations.summary }}'
        severity: '{{ .CommonLabels.severity }}'
        details:
          service: '{{ .CommonLabels.service }}'
          alert: '{{ .GroupLabels.alertname }}'
```

---

## ã‚¢ãƒ©ãƒ¼ãƒˆå¯¾å¿œ Runbook

### Runbook ã®æ§‹æˆ

å„ã‚¢ãƒ©ãƒ¼ãƒˆã«ã¯å¯¾å¿œæ‰‹é †ï¼ˆRunbookï¼‰ã‚’ç”¨æ„:

```markdown
# PostgreSQLDown Runbook

## æ¦‚è¦
PostgreSQLãŒåœæ­¢ã—ã¦ã„ã‚‹çŠ¶æ…‹

## å½±éŸ¿
- å…¨ã‚µãƒ¼ãƒ“ã‚¹ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯
- èªè¨¼ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ãŒä¸å¯

## ç·Šæ€¥å¯¾å¿œï¼ˆ5åˆ†ä»¥å†…ï¼‰
1. PostgreSQLã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ…‹ç¢ºèª
   ```bash
   docker ps | grep postgres
   docker logs postgres
   ```

2. å†èµ·å‹•è©¦è¡Œ
   ```bash
   cd ai-micro-postgres
   docker compose restart
   ```

3. æ¥ç¶šç¢ºèª
   ```bash
   docker exec postgres psql -U postgres -c "SELECT 1"
   ```

## æ ¹æœ¬åŸå› èª¿æŸ»
- ãƒ­ã‚°ç¢ºèª: `/var/lib/docker/containers/.../postgres.log`
- ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºèª: `df -h`
- ãƒ¡ãƒ¢ãƒªç¢ºèª: `free -h`

## ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- 15åˆ†ã§å¾©æ—§ã—ãªã„å ´åˆ â†’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆã«é€£çµ¡
```

### Runbook ã®ä¿å­˜å ´æ‰€

```
docs/runbooks/
  â”œâ”€â”€ postgres-down.md
  â”œâ”€â”€ redis-down.md
  â”œâ”€â”€ auth-service-down.md
  â”œâ”€â”€ high-error-rate.md
  â””â”€â”€ high-latency.md
```

---

## ã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ

### æ‰‹å‹•ãƒ†ã‚¹ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡

```bash
# AlertManagerã«ãƒ†ã‚¹ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
curl -X POST http://localhost:9093/api/v1/alerts \
  -H "Content-Type: application/json" \
  -d '[
    {
      "labels": {
        "alertname": "TestAlert",
        "severity": "warning",
        "service": "test"
      },
      "annotations": {
        "summary": "This is a test alert",
        "description": "Testing alert notification"
      }
    }
  ]'
```

### ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«æ¤œè¨¼

```bash
# Prometheusãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
promtool check rules alerts/*.yml

# ç‰¹å®šã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç«ã™ã‚‹ã‹ç¢ºèª
promtool query instant http://localhost:9090 \
  'up{job="auth-service"} == 0'
```

---

## ã‚¢ãƒ©ãƒ¼ãƒˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### ã‚¢ãƒ©ãƒ¼ãƒˆè‡ªä½“ã®ç›£è¦–

```yaml
# AlertManagerãƒ¡ãƒˆãƒªã‚¯ã‚¹
alertmanager_notifications_total  # é€šçŸ¥é€ä¿¡ç·æ•°
alertmanager_notifications_failed_total  # é€šçŸ¥å¤±æ•—æ•°
alertmanager_alerts  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆæ•°
alertmanager_silences  # ã‚µã‚¤ãƒ¬ãƒ³ã‚¹æ•°
```

### ã‚¢ãƒ©ãƒ¼ãƒˆå“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹

```yaml
è¨ˆæ¸¬ã™ã¹ããƒ¡ãƒˆãƒªã‚¯ã‚¹:
  - MTTD (Mean Time To Detect): éšœå®³æ¤œçŸ¥ã¾ã§ã®å¹³å‡æ™‚é–“
  - MTTR (Mean Time To Resolve): è§£æ±ºã¾ã§ã®å¹³å‡æ™‚é–“
  - False Positive Rate: èª¤æ¤œçŸ¥ç‡
  - Alert Fatigue Index: ã‚¢ãƒ©ãƒ¼ãƒˆç–²ã‚ŒæŒ‡æ•°
```

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ã‚¢ãƒ©ãƒ¼ãƒˆã¯ç—‡çŠ¶ã«å¯¾ã—ã¦ã€åŸå› ã§ã¯ãªã„

```yaml
# âŒ æ‚ªã„ä¾‹
- alert: DiskUsageHigh
  expr: disk_usage > 80%

# âœ… è‰¯ã„ä¾‹
- alert: ServiceDegradedDueToHighDiskUsage
  expr: disk_usage > 80% AND service_error_rate > 5%
```

### 2. ã‚¢ãƒ©ãƒ¼ãƒˆã«ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã‚‹

```yaml
annotations:
  summary: "PostgreSQL connection limit approaching"
  description: "Current: {{ $value }}%, Max: 100 connections"
  impact: "New connections may be rejected"
  runbook_url: "https://docs.example.com/runbooks/postgres-connections"
  dashboard_url: "https://grafana.example.com/d/postgres"
```

### 3. ã‚¢ãƒ©ãƒ¼ãƒˆé–¾å€¤ã®èª¿æ•´

```yaml
# åˆæœŸè¨­å®š
threshold: 80%

# é‹ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«èª¿æ•´
# - éå»1ãƒ¶æœˆã®æœ€å¤§å€¤: 75%
# - ä½™è£•ã‚’æŒãŸã›ã¦: 85%ã«èª¿æ•´
threshold: 85%
```

### 4. ã‚µã‚¤ãƒ¬ãƒ³ã‚¹æ©Ÿèƒ½ã®æ´»ç”¨

```bash
# ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä¸­ã¯ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ã‚µã‚¤ãƒ¬ãƒ³ã‚¹
amtool silence add \
  alertname=PostgreSQLDown \
  --duration=2h \
  --comment="Scheduled maintenance"
```

---

## å‚è€ƒè³‡æ–™

- [03-monitoring.md](./03-monitoring.md) - ç›£è¦–è¨­è¨ˆ
- [04-logging.md](./04-logging.md) - ãƒ­ã‚°è¨­è¨ˆ
- [06-troubleshooting.md](./06-troubleshooting.md) - ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- [Prometheus Alerting Rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)
- [AlertManager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)

---

**å¤‰æ›´å±¥æ­´**:

- 2025-09-30: åˆç‰ˆä½œæˆ